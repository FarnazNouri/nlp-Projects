{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36016251",
   "metadata": {},
   "source": [
    "# Regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e00738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95850ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat1 = ' my num is 123456789 and you are age (50)ask a lot of questions addD_45@gmail.com 4657008755 donndh I dont want to give you my number (123)-455-2333,email hgujk_@gmk.ca do you really wanna know? 234423424 ok: 333-544-6655 order #23449 and order no 3444 is not delivered order 778'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71bf5ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123456789', '465700875', ' (123)-455-2333', '234423424', ' 333-544-6655']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone = '\\d{9}| \\(\\d{3}\\)-\\d{3}-\\d{4}| \\d{3}-\\d{3}-\\d{4}'\n",
    "matches = re.findall(phone, chat1)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fbffd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['addD_45@gmail.com', 'hgujk_@gmk.ca']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = '[a-z0-9A-Z_]*@[a-z0-9A-Z]*\\.[a-zA-Z]*'\n",
    "matches = re.findall(email, chat1)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84220c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23449', '3444', '778']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = 'order[^\\d]*(\\d*)'\n",
    "matches = re.findall(pattern, chat1)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f59f9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki ='''\n",
    "Born\tThomas Jeffrey Hanks\n",
    "July 9, 1956 (age 69)\n",
    "Concord, California, U.S.\n",
    "Citizenship\tUnited States\n",
    "Greece (since 2020)[1]\n",
    "Alma mater\tChabot College\n",
    "Occupations\t\n",
    "Actor filmmaker\n",
    "Years active\t1977–present\n",
    "Works\tFull list\n",
    "Spouses\t\n",
    "Samantha Lewes\n",
    "​\n",
    "​(m. 1978; div. 1987)​\n",
    "Rita Wilson ​(m. 1988)​\n",
    "Children\t4, including Colin and Chet\n",
    "Relatives\t\n",
    "Jim Hanks (brother)\n",
    "Larry Hanks (brother)\n",
    "Lincoln family (through Nancy Hanks)\n",
    "Awards\tFull list\n",
    "Signature\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "324e254f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['69']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = 'age (\\d+)'\n",
    "matches = re.findall(pattern,wiki)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "162c527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\tThomas Jeffrey Hanks']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = 'Born(.*)'\n",
    "matches = re.findall(pattern,wiki)\n",
    "matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98de2d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['July 9, 1956 ']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = 'Born.*\\n(.*)\\(age'\n",
    "re.findall(pattern, wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4a380",
   "metadata": {},
   "source": [
    "### 1. Extract all twitter handles from following text. Twitter handle is the text that appears after https://twitter.com/ and is a single word. Also it contains only alpha numeric characters i.e. A-Z a-z , o to 9 and underscore _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b36596c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elonmusk', 'teslarati', 'dummy_tesla', 'dummy_2_tesla']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk, more information \n",
    "on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers \n",
    "for tesla related news,\n",
    "https://twitter.com/teslarati\n",
    "https://twitter.com/dummy_tesla\n",
    "https://twitter.com/dummy_2_tesla\n",
    "'''\n",
    "pattern = 'https:\\/\\/twitter\\.com\\/([\\_a-z0-9]*)' # todo: type your regex here\n",
    "\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14437d1",
   "metadata": {},
   "source": [
    "### 2. Extract Concentration Risk Types. It will be a text that appears after \"Concentration Risk:\", In below example, your regex should extract these two strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "437a74c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Credit Risk', ' Supply Risk']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\t\n",
    "text = '''\n",
    "Concentration of Risk: Credit Risk\n",
    "Financial instruments that potentially subject us to a concentration of credit risk consist of cash, cash equivalents, marketable securities,\n",
    "restricted cash, accounts receivable, convertible note hedges, and interest rate swaps. Our cash balances are primarily invested in money market funds\n",
    "or on deposit at high credit quality financial institutions in the U.S. These deposits are typically in excess of insured limits. As of September 30, 2021\n",
    "and December 31, 2020, no entity represented 10% or more of our total accounts receivable balance. The risk of concentration for our convertible note\n",
    "hedges and interest rate swaps is mitigated by transacting with several highly-rated multinational banks.\n",
    "Concentration of Risk: Supply Risk\n",
    "We are dependent on our suppliers, including single source suppliers, and the inability of these suppliers to deliver necessary components of our\n",
    "products in a timely manner at prices, quality levels and volumes acceptable to us, or our inability to efficiently manage these components from these\n",
    "suppliers, could have a material adverse effect on our business, prospects, financial condition and operating results.\n",
    "'''\n",
    "pattern = 'Concentration of Risk:([ a-z0-9A-Z]*)' # todo: type your regex here\n",
    "\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e799d9b",
   "metadata": {},
   "source": [
    "### 3. Companies in europe reports their financial numbers of semi annual basis and you can have a document like this. To exatract quarterly and semin annual period you can use a regex as shown below\n",
    "\n",
    "Hint: you need to use (?:) here to match everything enclosed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dcc4e37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FY2021 Q1', 'FY2021 S1']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
    "BMW's gross cost of operating vehicles in FY2021 S1 was $8 billion.\n",
    "'''\n",
    "\n",
    "pattern = 'FY\\d{4} (?:Q[1-4]|S[1-2])' # todo: type your regex here\n",
    "matches = re.findall(pattern, text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e23eb",
   "metadata": {},
   "source": [
    "### 4. Extract only financial numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion. \n",
    "In previous quarter i.e. FY2020 Q4 it was $3 billion.\n",
    "'''\n",
    "\n",
    "pattern = '\\$[1-9\\.]+'\n",
    "matches = re.findall(pattern, text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61842113",
   "metadata": {},
   "source": [
    "### 5. Extract periods and financial numbers both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f791a5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2021 Q1', '4.85'), ('2020 Q4', '3')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion. \n",
    "In previous quarter i.e. FY2020 Q4 it was $3 billion.\n",
    "'''\n",
    "pattern = 'FY(\\d{4} Q[1-4])[^\\$]+\\$([0-9\\.]+)'\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367022c",
   "metadata": {},
   "source": [
    "## re.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "99a8d3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(51, 70), match='FY2021 Q1 was $4.85'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion. \n",
    "In previous quarter i.e. FY2020 Q4 it was $3 billion.\n",
    "'''\n",
    "pattern = 'FY(\\d{4} Q[1-4])[^\\$]+\\$([0-9\\.]+)'\n",
    "\n",
    "matches = re.search(pattern, text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f10b4260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2021 Q1', '4.85')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef5461",
   "metadata": {},
   "source": [
    "# SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc5c822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farnaznouri/Documents/Leetcode/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576dc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66380e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ce786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farnaznouri/Documents/Leetcode/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93bc57",
   "metadata": {},
   "source": [
    "## Sentence tokenization using spacy (Object Oriented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2c794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav nouri of united states.\n",
      "Ali loves new jersey\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Dr. Strange loves pav nouri of united states. Ali loves new jersey\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13ad3c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "nouri\n",
      "of\n",
      "united\n",
      "states\n",
      ".\n",
      "Ali\n",
      "loves\n",
      "new\n",
      "jersey\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfd20e",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b4a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/farnaznouri/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edbf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"Dr. Strange loves pav nouri of united states. Ali loves new jersey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f966685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "nouri\n",
      "of\n",
      "united\n",
      "states\n",
      ".\n",
      "Ali\n",
      "loves\n",
      "new\n",
      "jersey\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Dr. Strange loves pav nouri of united states. Ali loves new jersey\")\n",
    "for tokens in doc:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d76bb611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Let\n",
      "'s\n",
      "got\n",
      "to\n",
      "N.Y.\n",
      "!\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('''\"Let's got to N.Y.!\"''')\n",
    "for tokens in doc:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f05b49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8787ddb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c05ea7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Tony gave two $ to peter\")\n",
    "token0 = doc[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79e9fc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30678f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc548ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b74d3f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[2]\n",
    "token2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5877f7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf520d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3 = doc[3]\n",
    "token3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a95707f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52e37fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> index: 0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "gave ==> index: 1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "two ==> index: 2 is_alpha: True is_punct: False like_num: True is_currency: False\n",
      "$ ==> index: 3 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "to ==> index: 4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "peter ==> index: 5 is_alpha: True is_punct: False like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,'==>', 'index:', token.i, 'is_alpha:', token.is_alpha,\n",
    "          'is_punct:', token.is_punct,\n",
    "          'like_num:', token.like_num,\n",
    "          'is_currency:', token.is_currency,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce8db27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the list of the student emails:\\n',\n",
       " 'all@gmail.com 8790077 lkkijohhf\\n',\n",
       " 'fhhf@koi.com jdhhkjns 88\\n',\n",
       " '1. emili@kk.com\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('students.txt') as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e6c5806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the list of the student emails:\\n all@gmail.com 8790077 lkkijohhf\\n fhhf@koi.com jdhhkjns 88\\n 1. emili@kk.com\\n \\n \\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "caaea1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all@gmail.com', 'fhhf@koi.com', 'emili@kk.com']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d9b46fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6bee8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"}\n",
    "])\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde196c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gimme double cheese extra large healthy pizza.\n",
      "Ali is here.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza. Ali is here.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54515c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x13aff3380>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6a7e527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d44bb69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gimme double cheese extra large healthy pizza.\n",
      "Dr. Ali is here.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza. Dr. Ali is here.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6973e",
   "metadata": {},
   "source": [
    "\n",
    "### Collecting dataset websites from a book paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13b5a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1be0543d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "data_web = [token.text for token in doc if token.like_url]\n",
    "data_web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93310f76",
   "metadata": {},
   "source": [
    "### Figure out all transactions from this text with amount and currency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "01dcb9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tony gave two $ to Peter, Bruce gave 500 € to Steve\")\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token.text, doc[token.i+1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28bd944",
   "metadata": {},
   "source": [
    "# NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e3a19b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1aacf1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ad004bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain | PROPN | Captain | NNP |\n",
      "america | PROPN | america | NNP |\n",
      "ate | VERB | eat | VBD |\n",
      "100 | NUM | 100 | CD |\n",
      "$ | NUM | $ | CD |\n",
      "of | ADP | of | IN |\n",
      "samosa | PROPN | samosa | NNP |\n",
      ". | PUNCT | . | . |\n",
      "Then | ADV | then | RB |\n",
      "he | PRON | he | PRP |\n",
      "said | VERB | say | VBD |\n",
      "I | PRON | I | PRP |\n",
      "can | AUX | can | MD |\n",
      "do | VERB | do | VB |\n",
      "this | PRON | this | DT |\n",
      "all | DET | all | DT |\n",
      "day | NOUN | day | NN |\n",
      ". | PUNCT | . | . |\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "for token in doc:\n",
    "    print(token,'|', token.pos_,'|', token.lemma_,'|', token.tag_,'|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96478938",
   "metadata": {},
   "source": [
    "## Ner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d5ccd737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "78b2d707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7d84b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Twitter Inc  |  PERSON  |  People, including fictional\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Tesla Inc is going to acquire Twitter Inc for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ab298da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c39086ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is going to"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d0236",
   "metadata": {},
   "source": [
    "## Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "669cda9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "47cb7439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc | ORG\n",
      "acquire | ORG\n",
      "Twitter Inc | PERSON\n",
      "$45 billion | MONEY\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "s1 = Span(doc, 0, 1, label= \"ORG\")\n",
    "s2 = Span(doc, 5, 6, label=\"ORG\")\n",
    "doc.set_ents([s1,s2], default='unmodified')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, '|', ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39ca17",
   "metadata": {},
   "source": [
    "### If I use blank pipeline I won't have the same result as when I use \"en_core_web_sm\" pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b9c0c529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain |  |  |  |\n",
      "america |  |  |  |\n",
      "ate |  |  |  |\n",
      "100 |  |  |  |\n",
      "$ |  |  |  |\n",
      "of |  |  |  |\n",
      "samosa |  |  |  |\n",
      ". |  |  |  |\n",
      "Then |  |  |  |\n",
      "he |  |  |  |\n",
      "said |  |  |  |\n",
      "I |  |  |  |\n",
      "can |  |  |  |\n",
      "do |  |  |  |\n",
      "this |  |  |  |\n",
      "all |  |  |  |\n",
      "day |  |  |  |\n",
      ". |  |  |  |\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,'|', token.pos_,'|', token.lemma_,'|', token.tag_,'|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a4658",
   "metadata": {},
   "source": [
    "## Stemming and Lammatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e48f6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b9c04f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "Eat | eat\n",
      "ate | ate\n",
      "eat | eat\n",
      "ability | abil\n",
      "meeting | meet\n",
      "rafting | raft\n",
      "better | better\n"
     ]
    }
   ],
   "source": [
    "words = ['eating', 'Eat', 'ate', 'eat', 'ability', 'meeting', 'rafting', 'better']\n",
    "for word in words:\n",
    "    print(word, '|', stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "56427d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat |\n",
      "Eat | Eat |\n",
      "ate | eat |\n",
      "eat | eat |\n",
      "ability | ability |\n",
      "meeting | meeting |\n",
      "rafting | raft |\n",
      "better | well |\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"eating Eat ate eat ability meeting rafting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, '|', token.lemma_, '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b786c74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Bro\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brah\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhusted | exhuste\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhusted\")\n",
    "for token in doc:\n",
    "    print(token, '|', token.lemma_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "384eaaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c9165",
   "metadata": {},
   "source": [
    "## 'attribute_ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0cc53f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhusted | exhuste\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "ar.add([[{\"Text\":\"Bro\"}],[{\"Text\":\"Brah\"}]], {\"LEMMA\":\"Brother\"})\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhusted\")\n",
    "for token in doc:\n",
    "    print(token, '|', token.lemma_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb97cc",
   "metadata": {},
   "source": [
    "## POS -> Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "53803027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farnaz | Farnaz | PROPN | NNP noun, proper singular\n",
      "flew | fly | VERB | VBD verb, past tense\n",
      "to | to | ADP | IN conjunction, subordinating or preposition\n",
      "Mars | Mars | PROPN | NNP noun, proper singular\n",
      "and | and | CCONJ | CC conjunction, coordinating\n",
      "he | he | PRON | PRP pronoun, personal\n",
      "ate | eat | VERB | VBD verb, past tense\n",
      "pizza | pizza | NOUN | NN noun, singular or mass\n",
      "over | over | ADP | IN conjunction, subordinating or preposition\n",
      "there | there | ADV | RB adverb\n",
      ". | . | PUNCT | . punctuation mark, sentence closer\n",
      "Wow | wow | INTJ | UH interjection\n",
      "! | ! | PUNCT | . punctuation mark, sentence closer\n",
      "she | she | PRON | PRP pronoun, personal\n",
      "is | be | AUX | VBZ verb, 3rd person singular present\n",
      "always | always | ADV | RB adverb\n",
      "hungry | hungry | ADJ | JJ adjective (English), other noun-modifier (Chinese)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Farnaz flew to Mars and he ate pizza over there. Wow! she is always hungry\")\n",
    "for token in doc:\n",
    "    print(token, '|', token.lemma_, '|',token.pos_,'|', token.tag_, spacy.explain(token.tag_))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4948a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('''Some text\" could refer to several things, including placeholder text used in design, various forms of written content, or even internet abbreviations. \"Lorem ipsum\", for example, is a common placeholder text used to demonstrate the visual form of a design. Additionally, \"text\" can encompass a wide range of written materials, from books and articles to scripts, songs, and even visual elements like advertisements. Finally, in online communication, abbreviations like \"LOL\", \"FYI\", and \"ASAP\" are frequently used to shorten messages''')\n",
    "filtered_token = []\n",
    "for token in doc:\n",
    "    if token.pos_ not in ['SPACE', 'X', 'PUNCT']:\n",
    "        filtered_token.append(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6b47bf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Some,\n",
       " text,\n",
       " could,\n",
       " refer,\n",
       " to,\n",
       " several,\n",
       " things,\n",
       " including,\n",
       " placeholder,\n",
       " text,\n",
       " used,\n",
       " in,\n",
       " design,\n",
       " various,\n",
       " forms,\n",
       " of,\n",
       " written,\n",
       " content,\n",
       " or,\n",
       " even]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_token[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "287ca411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{90: 5,\n",
       " 92: 26,\n",
       " 97: 27,\n",
       " 87: 4,\n",
       " 100: 10,\n",
       " 85: 11,\n",
       " 84: 7,\n",
       " 89: 4,\n",
       " 86: 5,\n",
       " 96: 5,\n",
       " 94: 2}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aa9bf292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[100].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ccb5a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET | 5\n",
      "NOUN | 26\n",
      "PUNCT | 27\n",
      "AUX | 4\n",
      "VERB | 10\n",
      "ADP | 11\n",
      "ADJ | 7\n",
      "CCONJ | 4\n",
      "ADV | 5\n",
      "PROPN | 5\n",
      "PART | 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text,'|',v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da1e25",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "#### Convert these list of words into base form using Stemming and Lemmatization and observe the transformations\n",
    "Write a short note on the words that have different base words using stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686d0dc",
   "metadata": {},
   "source": [
    "##### Run this cell to import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let import necessary libraries and create the object\n",
    "\n",
    "#for nltk\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#downloading all neccessary packages related to nltk\n",
    "nltk.download('all')\n",
    "\n",
    "\n",
    "#for spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dcb1ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using stemming in nltk\n",
    "lst_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ebcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using lemmatization in spacy\n",
    "doc = nlp(\"running painting walking dressing likely children who good ate fishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5ffb72a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running | run\n",
      "painting | paint\n",
      "walking | walk\n",
      "dressing | dress\n",
      "likely | like\n",
      "children | children\n",
      "whom | whom\n",
      "good | good\n",
      "ate | ate\n",
      "fishing | fish\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "for word in lst_words:\n",
    "    print(word,'|',stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d9bad209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running | run\n",
      "painting | painting\n",
      "walking | walking\n",
      "dressing | dress\n",
      "likely | likely\n",
      "children | child\n",
      "who | who\n",
      "good | good\n",
      "ate | eat\n",
      "fishing | fish\n"
     ]
    }
   ],
   "source": [
    "# lemmatization:\n",
    "for token in doc:\n",
    "    print(token, '|', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28509b3",
   "metadata": {},
   "source": [
    "## important exercises\n",
    "### Exercise2:\n",
    "\n",
    "#### convert the given text into it's base form using both stemming and lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a8ff2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Latha is very multi talented girl.She is good at many skills like dancing, running, singing, playing.She also likes eating Pav Bhagi. she has a \n",
    "habit of fishing and swimming too.Besides all this, she is a wonderful at cooking too.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b2470d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latha be very multi talented girl . she be good at many skill like dancing , running , singing , play . she also like eat Pav Bhagi . she have a \n",
      " habit of fishing and swim too . besides all this , she be a wonderful at cook too . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "words = []\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    word = token.lemma_\n",
    "    words.append(word)\n",
    "\n",
    "final_base_text = ' '.join(words)\n",
    "print(final_base_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d3d76e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latha is veri multi talent girl.sh is good at mani skill like danc , run , sing , playing.sh also like eat pav bhagi . she ha a habit of fish and swim too.besid all thi , she is a wonder at cook too .\n"
     ]
    }
   ],
   "source": [
    "#using stemming in nltk\n",
    "\n",
    "#step1: Word tokenizing\n",
    "all_word_token = nltk.word_tokenize(text)\n",
    "\n",
    "#step2: getting the base from each token using stemmer\n",
    "all_base_words = []\n",
    "\n",
    "for token in all_word_token:\n",
    "    base = stemmer.stem(token)\n",
    "    all_base_words.append(base)\n",
    "\n",
    "#step3: joining all words in a list into string using 'join()\n",
    "final_base_text = ' '.join(all_base_words)\n",
    "print(final_base_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2be0811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5548238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
